{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Location</th>\n",
       "      <th>Resource Type</th>\n",
       "      <th>floor</th>\n",
       "      <th>Department</th>\n",
       "      <th>seatName</th>\n",
       "      <th>Space</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India</td>\n",
       "      <td>Tower 2</td>\n",
       "      <td>Desk</td>\n",
       "      <td>Level 9</td>\n",
       "      <td>Sky Branch</td>\n",
       "      <td>B5_9_1_02</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>Tower 2</td>\n",
       "      <td>Desk</td>\n",
       "      <td>Level 9</td>\n",
       "      <td>Sky Branch</td>\n",
       "      <td>B5_9_2_02</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>Tower 2</td>\n",
       "      <td>Desk</td>\n",
       "      <td>Level 9</td>\n",
       "      <td>Sky Branch</td>\n",
       "      <td>B5_9_2_07</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country Location Resource Type    floor  Department   seatName  Space  \\\n",
       "0   India  Tower 2          Desk  Level 9  Sky Branch  B5_9_1_02      0   \n",
       "1   India  Tower 2          Desk  Level 9  Sky Branch  B5_9_2_02      0   \n",
       "2   India  Tower 2          Desk  Level 9  Sky Branch  B5_9_2_07      0   \n",
       "\n",
       "    Datetime  \n",
       "0 2018-03-01  \n",
       "1 2018-03-01  \n",
       "2 2018-03-01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "data=pickle.load(open('processed_data.p', \"rb\" ))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n"
     ]
    }
   ],
   "source": [
    "trial_data=data.loc[data['seatName']=='B5_9_1_03']\n",
    "print(len(trial_data))\n",
    "trial_data=trial_data.sort_values(['Datetime'])\n",
    "trial_data=trial_data.drop(['Country','Location','Resource Type','floor','Department','seatName','Datetime'],axis=1)\n",
    "trial_data=trial_data.values\n",
    "#trial_data.set_index('Datetime',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6f69330515dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "display(trial_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3794 3794 670\n"
     ]
    }
   ],
   "source": [
    "train_size=int(len(trial_data)*0.85)\n",
    "train_data,test_data=trial_data[0:train_size,:], trial_data[train_size:len(trial_data),:]\n",
    "print(train_size,len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train_data, look_back)\n",
    "print(X_train)\n",
    "X_test, y_test = create_dataset(test_data, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3792 samples, validate on 668 samples\n",
      "Epoch 1/100\n",
      "3792/3792 [==============================] - 2s 409us/step - loss: 0.6528 - acc: 0.9536 - val_loss: 0.5957 - val_acc: 0.9985\n",
      "Epoch 2/100\n",
      "3792/3792 [==============================] - 1s 175us/step - loss: 0.5547 - acc: 0.9536 - val_loss: 0.4766 - val_acc: 0.9985\n",
      "Epoch 3/100\n",
      "3792/3792 [==============================] - 1s 179us/step - loss: 0.4422 - acc: 0.9536 - val_loss: 0.3517 - val_acc: 0.9985\n",
      "Epoch 4/100\n",
      "3792/3792 [==============================] - 1s 176us/step - loss: 0.3373 - acc: 0.9536 - val_loss: 0.2475 - val_acc: 0.9985\n",
      "Epoch 5/100\n",
      "3792/3792 [==============================] - 1s 190us/step - loss: 0.2568 - acc: 0.9536 - val_loss: 0.1738 - val_acc: 0.9985\n",
      "Epoch 6/100\n",
      "3792/3792 [==============================] - 1s 181us/step - loss: 0.2015 - acc: 0.9536 - val_loss: 0.1251 - val_acc: 0.9985\n",
      "Epoch 7/100\n",
      "3792/3792 [==============================] - 1s 189us/step - loss: 0.1645 - acc: 0.9536 - val_loss: 0.0938 - val_acc: 0.9985\n",
      "Epoch 8/100\n",
      "3792/3792 [==============================] - 1s 180us/step - loss: 0.1391 - acc: 0.9536 - val_loss: 0.0732 - val_acc: 0.9985\n",
      "Epoch 9/100\n",
      "3792/3792 [==============================] - 1s 181us/step - loss: 0.1210 - acc: 0.9536 - val_loss: 0.0591 - val_acc: 0.9985\n",
      "Epoch 10/100\n",
      "3792/3792 [==============================] - 1s 181us/step - loss: 0.1078 - acc: 0.9539 - val_loss: 0.0491 - val_acc: 0.9970\n",
      "Epoch 11/100\n",
      "3792/3792 [==============================] - 1s 178us/step - loss: 0.0981 - acc: 0.9815 - val_loss: 0.0420 - val_acc: 0.9970\n",
      "Epoch 12/100\n",
      "3792/3792 [==============================] - 1s 186us/step - loss: 0.0912 - acc: 0.9815 - val_loss: 0.0367 - val_acc: 0.9970\n",
      "Epoch 13/100\n",
      "3792/3792 [==============================] - 1s 185us/step - loss: 0.0861 - acc: 0.9815 - val_loss: 0.0327 - val_acc: 0.9970\n",
      "Epoch 14/100\n",
      "3792/3792 [==============================] - 1s 182us/step - loss: 0.0826 - acc: 0.9815 - val_loss: 0.0296 - val_acc: 0.9970\n",
      "Epoch 15/100\n",
      "3792/3792 [==============================] - 1s 186us/step - loss: 0.0804 - acc: 0.9815 - val_loss: 0.0274 - val_acc: 0.9970\n",
      "Epoch 16/100\n",
      "3792/3792 [==============================] - 1s 186us/step - loss: 0.0788 - acc: 0.9815 - val_loss: 0.0257 - val_acc: 0.9970\n",
      "Epoch 17/100\n",
      "3792/3792 [==============================] - 1s 187us/step - loss: 0.0777 - acc: 0.9815 - val_loss: 0.0243 - val_acc: 0.9970\n",
      "Epoch 18/100\n",
      "3792/3792 [==============================] - 1s 191us/step - loss: 0.0769 - acc: 0.9815 - val_loss: 0.0232 - val_acc: 0.9970\n",
      "Epoch 19/100\n",
      "3792/3792 [==============================] - 1s 185us/step - loss: 0.0763 - acc: 0.9815 - val_loss: 0.0223 - val_acc: 0.9970\n",
      "Epoch 20/100\n",
      "3792/3792 [==============================] - 1s 184us/step - loss: 0.0760 - acc: 0.9815 - val_loss: 0.0217 - val_acc: 0.9970\n",
      "Epoch 21/100\n",
      "3792/3792 [==============================] - 1s 183us/step - loss: 0.0757 - acc: 0.9815 - val_loss: 0.0211 - val_acc: 0.9970\n",
      "Epoch 22/100\n",
      "3792/3792 [==============================] - 1s 182us/step - loss: 0.0755 - acc: 0.9815 - val_loss: 0.0206 - val_acc: 0.9970\n",
      "Epoch 23/100\n",
      "3792/3792 [==============================] - 1s 179us/step - loss: 0.0754 - acc: 0.9815 - val_loss: 0.0204 - val_acc: 0.9970\n",
      "Epoch 24/100\n",
      "3792/3792 [==============================] - 1s 189us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0202 - val_acc: 0.9970\n",
      "Epoch 25/100\n",
      "3792/3792 [==============================] - 1s 188us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0200 - val_acc: 0.9970\n",
      "Epoch 26/100\n",
      "3792/3792 [==============================] - 1s 187us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0197 - val_acc: 0.9970\n",
      "Epoch 27/100\n",
      "3792/3792 [==============================] - 1s 198us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0196 - val_acc: 0.9970\n",
      "Epoch 28/100\n",
      "3792/3792 [==============================] - 1s 201us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0195 - val_acc: 0.9970\n",
      "Epoch 29/100\n",
      "3792/3792 [==============================] - 1s 199us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0194 - val_acc: 0.9970\n",
      "Epoch 30/100\n",
      "3792/3792 [==============================] - 1s 179us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0193 - val_acc: 0.9970\n",
      "Epoch 31/100\n",
      "3792/3792 [==============================] - 1s 175us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0193 - val_acc: 0.9970\n",
      "Epoch 32/100\n",
      "3792/3792 [==============================] - 1s 170us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0192 - val_acc: 0.9970\n",
      "Epoch 33/100\n",
      "3792/3792 [==============================] - 1s 169us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 34/100\n",
      "3792/3792 [==============================] - 1s 178us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 35/100\n",
      "3792/3792 [==============================] - 1s 170us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 36/100\n",
      "3792/3792 [==============================] - 1s 165us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 37/100\n",
      "3792/3792 [==============================] - 1s 168us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 38/100\n",
      "3792/3792 [==============================] - 1s 168us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 39/100\n",
      "3792/3792 [==============================] - 1s 168us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 40/100\n",
      "3792/3792 [==============================] - 1s 173us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 41/100\n",
      "3792/3792 [==============================] - 1s 171us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 42/100\n",
      "3792/3792 [==============================] - 1s 199us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0189 - val_acc: 0.9970\n",
      "Epoch 43/100\n",
      "3792/3792 [==============================] - 1s 172us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 44/100\n",
      "3792/3792 [==============================] - 1s 180us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 45/100\n",
      "3792/3792 [==============================] - 1s 175us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 46/100\n",
      "3792/3792 [==============================] - 1s 173us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 47/100\n",
      "3792/3792 [==============================] - 1s 181us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 48/100\n",
      "3792/3792 [==============================] - 1s 173us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 49/100\n",
      "3792/3792 [==============================] - 1s 188us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 50/100\n",
      "3792/3792 [==============================] - 1s 171us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 51/100\n",
      "3792/3792 [==============================] - 1s 170us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 52/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 53/100\n",
      "3792/3792 [==============================] - 1s 178us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 54/100\n",
      "3792/3792 [==============================] - 1s 169us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 55/100\n",
      "3792/3792 [==============================] - 1s 167us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 56/100\n",
      "3792/3792 [==============================] - 1s 177us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 57/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 58/100\n",
      "3792/3792 [==============================] - 1s 167us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0192 - val_acc: 0.9970\n",
      "Epoch 59/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3792/3792 [==============================] - 1s 180us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 61/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 62/100\n",
      "3792/3792 [==============================] - 1s 172us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 63/100\n",
      "3792/3792 [==============================] - 1s 171us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 64/100\n",
      "3792/3792 [==============================] - 1s 175us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 65/100\n",
      "3792/3792 [==============================] - 1s 171us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 66/100\n",
      "3792/3792 [==============================] - 1s 170us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 67/100\n",
      "3792/3792 [==============================] - 1s 170us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 68/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 69/100\n",
      "3792/3792 [==============================] - 1s 175us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 70/100\n",
      "3792/3792 [==============================] - 1s 174us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 71/100\n",
      "3792/3792 [==============================] - 1s 197us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 72/100\n",
      "3792/3792 [==============================] - 1s 204us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 73/100\n",
      "3792/3792 [==============================] - 1s 221us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 74/100\n",
      "3792/3792 [==============================] - 1s 206us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 75/100\n",
      "3792/3792 [==============================] - 1s 206us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 76/100\n",
      "3792/3792 [==============================] - 1s 202us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 77/100\n",
      "3792/3792 [==============================] - 1s 198us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 78/100\n",
      "3792/3792 [==============================] - 1s 187us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 79/100\n",
      "3792/3792 [==============================] - 1s 188us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 80/100\n",
      "3792/3792 [==============================] - 1s 189us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 81/100\n",
      "3792/3792 [==============================] - 1s 192us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 82/100\n",
      "3792/3792 [==============================] - 1s 202us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0189 - val_acc: 0.9970\n",
      "Epoch 83/100\n",
      "3792/3792 [==============================] - 1s 220us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 84/100\n",
      "3792/3792 [==============================] - 1s 192us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 85/100\n",
      "3792/3792 [==============================] - 1s 278us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 86/100\n",
      "3792/3792 [==============================] - 1s 191us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 87/100\n",
      "3792/3792 [==============================] - 1s 186us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 88/100\n",
      "3792/3792 [==============================] - 1s 185us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 89/100\n",
      "3792/3792 [==============================] - 1s 178us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 90/100\n",
      "3792/3792 [==============================] - 1s 184us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 91/100\n",
      "3792/3792 [==============================] - 1s 193us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0192 - val_acc: 0.9970\n",
      "Epoch 92/100\n",
      "3792/3792 [==============================] - 1s 237us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 93/100\n",
      "3792/3792 [==============================] - 1s 223us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 94/100\n",
      "3792/3792 [==============================] - 1s 282us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 95/100\n",
      "3792/3792 [==============================] - 1s 248us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.9970\n",
      "Epoch 96/100\n",
      "3792/3792 [==============================] - 1s 255us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 97/100\n",
      "3792/3792 [==============================] - 1s 200us/step - loss: 0.0753 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 98/100\n",
      "3792/3792 [==============================] - 1s 186us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 99/100\n",
      "3792/3792 [==============================] - 1s 188us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n",
      "Epoch 100/100\n",
      "3792/3792 [==============================] - 1s 187us/step - loss: 0.0752 - acc: 0.9815 - val_loss: 0.0190 - val_acc: 0.9970\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() got multiple values for argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-ff36cb3190e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m           validation_data=(X_test, y_test))\n\u001b[0;32m     26\u001b[0m score, acc = model.predict(X_test, y_test,\n\u001b[1;32m---> 27\u001b[1;33m                             batch_size=batch_size)\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got multiple values for argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Conv1D,Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def arch():\n",
    "    seq = Sequential()\n",
    "    seq.add(LSTM(4,input_shape=(1,look_back)))\n",
    "    #seq.add(LSTM(return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "    #seq.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(Dense(1, activation='sigmoid'))\n",
    "    seq.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return seq\n",
    "\n",
    "\n",
    "model=arch()\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 0s 92us/step\n",
      "Test score: 0.019003981066321184\n",
      "Test accuracy: 0.9970059880239521\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(X_test)\n",
    "plt.plot(testPredict)\n",
    "plt.plot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "plt.plot(trainPredict)\n",
    "plt.plot(trial_data['Space'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "trainPredictPlot = np.empty_like(trial_data)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(trial_data)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(trial_data)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
